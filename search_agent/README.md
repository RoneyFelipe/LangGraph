# ğŸ¤– Intelligent Assistant with LangGraph - AI Engineer Challenge

This repository contains the solution for the AI Engineer Technical Challenge. The project consists of a conversational agent capable of autonomously deciding when to respond using its internal knowledge, when to perform precise mathematical calculations, and when to search for real-time information on the web.

## ğŸš€ Features

- **Intelligent Routing:** The model semantically decides whether to respond with text, perform a calculation, or search the web.
- **Robustness (Retries):** Implemented retry logic using the `stamina` library to handle transient API errors in both LLM calls and mathematical operations.
- **Persistent Memory:** The conversation context is saved in a local SQLite database, allowing continuity across messages.
- **Integrated Tools:**
  - ğŸ§® **Calculator:** For exact mathematical operations with internal error handling.
  - ğŸŒ **Web Search (Tavily):** For up-to-date information (e.g., "What is the date today?", "Weather forecast").
  - ğŸ—‘ï¸ **Context Management:** A dedicated tool to clear the agent's memory directly from the database.

## ğŸ› ï¸ Tech Stack

- **Language:** Python 3.12
- **Orchestration:** [LangGraph](https://langchain-ai.github.io/langgraph/) (State graph and execution flow).
- **LLM:** Google Gemini 2.5 Flash (via `langchain-google-genai`).
- **Resilience:** [Stamina](https://stamina.hynek.me/) (For automatic retries on failures).
- **Search Engine:** Tavily AI.
- **Database:** SQLite (via LangGraph's `SqliteSaver`).

---

## ğŸ“‚ Project Structure

The code is modularized to ensure scalability and maintainability:

```text
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ graph.py     # Agent class definition, retry logic, and graph compilation
â”‚   â”œâ”€â”€ tools.py     # Calculator, Tavily Search, and Memory Reset tools
â”‚   â”œâ”€â”€ state.py     # AgentState definition and SQLite persistence setup
â”‚   â”œâ”€â”€ prompts.py   # Dynamic System Prompt (injects current date)
â”‚   â””â”€â”€ configs.py   # Environment variable management
â”œâ”€â”€ main.py          # Entry point (Interactive chat loop)
â”œâ”€â”€ requirements.txt # Project dependencies
â””â”€â”€ .env.example     # Template for required environment variables
```

---

## âš™ï¸ How to Run

### 1. Prerequisites
Ensure Python is installed. The use of a virtual environment is highly recommended.

### 2. Installation

Clone the repository and install dependencies using the `requirements.txt` file located in the root directory:

```bash
# Clone the repository
git clone <YOUR_REPO_URL>
cd <FOLDER_NAME>

# Create and activate virtual environment (Optional but recommended)
python -m venv venv

# On Windows:
# venv\Scripts\activate

# On Linux/Mac:
# source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 3. Configuration (.env)

Create a `.env` file in the project root and add your API keys. Make sure they match the names expected by `src/configs.py`:

```env
GEMINI_API_KEY="your_google_ai_studio_key"
TAVILY_API_KEY="your_tavily_key"
```

### 4. Running the Assistant

Execute the main file to start the chat loop:

```bash
python main.py
```

---

## ğŸ§  Implementation Logic

The solution was built using a custom `Agent` class that wraps the **LangGraph** state machine.

1.  **Dynamic Context:** The system prompt in `prompts.py` automatically injects the current date, ensuring the model knows "today's" date when performing searches.
2.  **Resilient Execution:**
    - The `call_gemini` method in `graph.py` is decorated with `@retry`, allowing up to 3 attempts if the API fails or times out.
    - The internal calculator logic in `tools.py` also implements retry logic to ensure stability.
3.  **Graph Flow:**
    - **Node `llm`**: Invokes the Gemini model with the bound tools.
    - **Conditional Edge**: Checks if the model output contains `tool_calls`.
    - **Node `action`**: Executes the requested tool (Math, Search, or Memory Reset) and returns the result to the graph.
4.  **Transaction Management:**
    - The `reset_memory` tool implements a dedicated SQLite connection to safely delete `checkpoints` and `writes` tables without conflicting with the main LangGraph thread.

---

## ğŸ’¡ Learnings and Next Steps

### What I learned
* **Production Readiness:** Adding retry mechanisms (`stamina`) is crucial for building reliable agents that don't crash on minor API hiccups.
* **Database Concurrency:** Learned to handle SQLite locking issues by creating independent connections when a Tool needs to perform administrative tasks (like deleting context) while the Agent holds the main connection.
* **State Management:** How to effectively map the `AgentState` to preserve conversation history across turns.

### What I would do differently with more time
* **User Interface:** Implement a graphical frontend using Streamlit or Chainlit.
* **Dockerization:** Create a `Dockerfile` for consistent deployment.
* **Streaming:** Implement token streaming to improve the perceived latency for the user.

---
